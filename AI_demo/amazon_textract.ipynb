{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import time\n",
    "from trp import Document\n",
    "from botocore.config import Config\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "import calendar\n",
    "import re\n",
    "import nltk\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from transformers import AutoTokenizer,GPT2TokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/cloudcraftz/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/cloudcraftz/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/cloudcraftz/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /home/cloudcraftz/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract text from a image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_text_extractor(path:str)->str:\n",
    "    '''\n",
    "    This function extracts text from a given image \n",
    "    '''\n",
    "    with open(path, 'rb') as image:\n",
    "        img = bytearray(image.read())\n",
    "        client = boto3.client('textract',region_name='ap-south-1')\n",
    "        response = client.detect_document_text(\n",
    "            Document={'Bytes': img}\n",
    "        )\n",
    "    text = \"\"\n",
    "    for item in response[\"Blocks\"]:\n",
    "        if item[\"BlockType\"] == \"LINE\":\n",
    "            text = text + \" \"+item[\"Text\"]\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Multi-Objective reward generalization: Improving performance of Deep Reinforcement Learning for selected applications in stock and cryptocurrency trading Federico Cornalba* Constantin Disselkamp**S Institute of Science and Technology Austria Davide Scassola** federico.cornalba@ist.ac.at Christopher Helf TRALITY GmbH constantin@trality.com davide@trality.com christopher@trality.com ABSTRACT the state variables and action (s,a determine the next state of the We investigate the potential of Multi-Objective, Deep Reinforce- environment that the algorithm will visit. The algorithm's ultimate ment Learning for stock and cryptocurrency trading. More specifi- goal is to maximise a cumulative reward (accounting for all actions cally, we build on the generalized setting à la Fontaine and Friedman taken in a given episode), which is in turn based on a pre-specified [6] (where the reward weighting mechanism is not specified a pri- state-action reward function r (assigning a numerical reward r(s, a) ori, but embedded in the learning process) by complementing it to every pair (s, a) of given state and action undertaken). The algo- with computational speed-ups, and adding the cumulative reward's rithm uses several episodes (each accounting for an exploration of discount factor to the learning process. the environment, and each ending when a pre-specified end-state Firstly, we verify that the resulting Multi-Objective algorithm is reached) to train its action-making (by progressively updating generalizes well, and we provide preliminary statistical evidence the so-called Q-values [14] throughout the episodes). The reward showing that its prediction is more stable than the correspond- function is both an integral part of the RL algorithm and the key ing Single-Objective strategy's. Secondly, we show that the Multi- metric against which its performance is measured. Objective algorithm has a clear edge over the corresponding Single- An intuitive example of a set of problems which a machine may Objective strategy when the reward mechanism is sparse (i.e., when tackle using the RL framework naturally occurs in gaming. For non-null feedback is infrequent over time). Finally, we discuss the instance, in the Atari 2600 games [10], the state of environment is generalization properties of the discount factor. represented by an image, the actions which the algorithm might The entirety of our code is provided in open source format. take correspond to directional movements of the game's charac- ters, and rewards are cumulative game scores. Generally speaking, CCS CONCEPTS the vast majority of problems which the RL paradigm was - origi- nally - applied to exhibit two clear features: firstly, (F1) they include Computing methodologies Reinforcement learning; Neu- robust and well-defined reward mechanisms (this provides the RL ral networks; Multi-task learning; Applied computing Eco- algorithms with the necessary quantitative feedback); secondly, (F2) nomics. they have a consistent, re-produceable and accessible environment. KEYWORDS This allows the RL algorithms to thoroughly explore the environ- ment in as many episodes as needed, and not to be affected by - Deep Reinforcement Learning, Multi-Objective Generalization, Multi- potentially destabilizing - environment inconsistencies and abrupt task learning, Stock Trading, Cryptocurrency Trading. variations in between episodes. Aside from the already mentioned field of gaming [10], RL techniques have been first deployed in 1 INTRODUCTION the field of robotics [7], of personalized recommendations [15] and Reinforcement Learning (RL) is a sub-field of Machine Learning resource management [9]. specifically designed to handle learning processes for problems The problems we are interested in, namely, those related to using which involve a dynamic interaction with a given underlying en- RL for profitable, risk-reduced trading of financial tools based on vironment. In a nutshell, a RL algorithm is supposed to learn to historical data, only partially satisfy features F1 & F2. We detail use the set of observable state variables S (describing the current this assertion by outlining three common problems in the field. state of the environment) to take the most appropriate admissible Firstly, (P1) the reward mechanisms which one might use to judge action a. Together with the environment's intrinsic stochasticity, the performance of a bot are not robust due the vast amount of noise coming from the financial environment. Secondly, (P2) the *These authors contributed equally to this work definition of the environment is often subjective, in the sense that Funded by the European Union's Horizon 2020 research and innovation programme under the Marie Sklodowska-Curie grant agreement No. 754411 one would need to decide which features are to be included in it Funded by the Austrian Research Promotion Agency (FFG) Corresponding Author 1\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_text_extractor('/home/cloudcraftz/Desktop/Test_3.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract text from a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Textract APIs used - \"start_document_text_detection\", \"get_document_text_detection\"\n",
    "def InvokeTextDetectJob(s3BucketName, objectName):\n",
    "    response = None\n",
    "    client = boto3.client('textract',region_name='ap-south-1')\n",
    "    \n",
    "    \n",
    "    response = client.start_document_text_detection(\n",
    "            DocumentLocation={\n",
    "                      'S3Object': {\n",
    "                                    'Bucket': s3BucketName,\n",
    "                                    'Name': objectName,\n",
    "                                }\n",
    "           })\n",
    "    return response[\"JobId\"]\n",
    "\n",
    "def CheckJobComplete(jobId):\n",
    "    time.sleep(5)\n",
    "    client = boto3.client('textract',region_name='ap-south-1')\n",
    "    response = client.get_document_text_detection(JobId=jobId)\n",
    "    status = response[\"JobStatus\"]\n",
    "    print(\"Job status: {}\".format(status))\n",
    "    while(status == \"IN_PROGRESS\"):\n",
    "        time.sleep(5)\n",
    "        response = client.get_document_text_detection(JobId=jobId)\n",
    "        status = response[\"JobStatus\"]\n",
    "        print(\"Job status: {}\".format(status))\n",
    "    return status\n",
    "\n",
    "def JobResults(jobId):\n",
    "    pages = []\n",
    "    client = boto3.client('textract',region_name='ap-south-1')\n",
    "    response = client.get_document_text_detection(JobId=jobId)\n",
    " \n",
    "    pages.append(response)\n",
    "    print(\"Resultset page recieved: {}\".format(len(pages)))\n",
    "    nextToken = None\n",
    "    if('NextToken' in response):\n",
    "        nextToken = response['NextToken']\n",
    "        while(nextToken):\n",
    "            response = client.get_document_text_detection(JobId=jobId, NextToken=nextToken)\n",
    "            pages.append(response)\n",
    "            print(\"Resultset page recieved: {}\".format(len(pages)))\n",
    "            nextToken = None\n",
    "            if('NextToken' in response):\n",
    "                nextToken = response['NextToken']\n",
    "    return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_text_extractor(name:str)->list:\n",
    "    ''' \n",
    "    This function takes the name of the document stored in AWS S3 and returns the actual text, the extracted text in a list.\n",
    "    '''\n",
    "\n",
    "    extracted_text_cleaned = []\n",
    "    extracted_text = []\n",
    "\n",
    "    # S3 Document Data\n",
    "    s3BucketName = \"fintech-sentiment-textract\"\n",
    "    documentName = name\n",
    "\n",
    "    # Function invokes\n",
    "    jobId = InvokeTextDetectJob(s3BucketName, documentName)\n",
    "    print(f\"Started job with id: {jobId} , name : {name}\")\n",
    "\n",
    "    if(CheckJobComplete(jobId)):\n",
    "        response = JobResults(jobId)\n",
    "        text_out = ''\n",
    "        for resultPage in response:\n",
    "            for item in resultPage[\"Blocks\"]:\n",
    "                if item[\"BlockType\"] == \"LINE\":\n",
    "                    text_out = text_out + item[\"Text\"]\n",
    "                    text_out = text_out.replace(\"\\'s\",\"\")\n",
    "\n",
    "    print(f\"For {name}, Text Extracted\")\n",
    "\n",
    "    extracted_text.append(text_out)\n",
    "\n",
    "    print(f'Text lemmatization and cleaning started.')\n",
    "\n",
    "    text_out = re.sub('[^a-zA-Z]', ' ', text_out)\n",
    "    text_out = text_out.lower()\n",
    "    text_out = ' '.join(text_out.split())\n",
    "\n",
    "    lemma = WordNetLemmatizer()\n",
    "    new_corpus = []\n",
    "    for i in text_out.split():\n",
    "        words = nltk.word_tokenize(i)\n",
    "        for word in words:\n",
    "            if word not in set(stopwords.words('english')):\n",
    "                new_corpus.append(lemma.lemmatize(word))\n",
    "\n",
    "    text_out = ' '.join(new_corpus)\n",
    "    print(f'Final text received')\n",
    "    extracted_text_cleaned.append(text_out)\n",
    "\n",
    "    return extracted_text, extracted_text_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started job with id: 59d790763227539a43b1eb31e6a4867b2bb1d17fb097ca60ad4fb7f2cb034d41 , name : RL-1.pdf\n",
      "Job status: SUCCEEDED\n",
      "Resultset page recieved: 1\n",
      "For RL-1.pdf, Text Extracted\n",
      "Text lemmatization and cleaning started.\n",
      "Final text received\n"
     ]
    }
   ],
   "source": [
    "extracted_text, extracted_text_cleaned = document_text_extractor('RL-1.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Multi-Objective reward generalization: Improving performanceof Deep Reinforcement Learning for selected applications instock and cryptocurrency tradingFederico Cornalba*Constantin Disselkamp**SInstitute of Science and Technology AustriaDavide Scassola**federico.cornalba@ist.ac.atChristopher HelfTRALITY GmbHconstantin@trality.comdavide@trality.comchristopher@trality.comABSTRACTthe state variables and action (s, determine the next state of theWe investigate the potential of Multi-Objective, Deep Reinforce-environment that the algorithm will visit. The algorithm ultimatement Learning for stock and cryptocurrency trading. More specifi-goal is to maximise a cumulative reward (accounting for all actionscally, we build on the generalized setting à la Fontaine and Friedmantaken in a given episode), which is in turn based on a pre-specified[6] (where the reward weighting mechanism is not specified a pri-state-action reward function r (assigning a numerical reward r(s, a)ori, but embedded in the learning process) by complementing itto every pair (s, a) of given state and action undertaken). The algo-with computational speed-ups, and adding the cumulative rewardrithm uses several episodes (each accounting for an exploration ofdiscount factor to the learning process.the environment, and each ending when a pre-specified end-stateFirstly, we verify that the resulting Multi-Objective algorithmis reached) to train its action-making (by progressively updatinggeneralizes well, and we provide preliminary statistical evidencethe so-called Q-values [14] throughout the episodes). The rewardshowing that its prediction is more stable than the correspond-function is both an integral part of the RL algorithm and the keying Single-Objective strategy. Secondly, we show that the Multi-metric against which its performance is measured.Objective algorithm has a clear edge over the corresponding Single-An intuitive example of a set of problems which a machine mayObjective strategy when the reward mechanism is sparse (i.e., whentackle using the RL framework naturally occurs in gaming. Fornon-null feedback is infrequent over time). Finally, we discuss theinstance, in the Atari 2600 games [10], the state of environment isgeneralization properties of the discount factor.represented by an image, the actions which the algorithm mightThe entirety of our code is provided in open source format.take correspond to directional movements of the game charac-ters, and rewards are cumulative game scores. Generally speaking,CCS CONCEPTSthe vast majority of problems which the RL paradigm was - origi-methodologies Reinforcement learning; Neu-nally - applied to exhibit two clear features: firstly, (F1) they includeComputingrobust and well-defined reward mechanisms (this provides the RLral networks; Multi-task learning; Applied computing Eco-algorithms with the necessary quantitative feedback); secondly, (F2)nomics.they have a consistent, re-produceable and accessible environment.KEYWORDSThis allows the RL algorithms to thoroughly explore the environ-ment in as many episodes as needed, and not to be affected by -Deep Reinforcement Learning, Multi-Objective Generalization, Multi-potentially destabilizing - environment inconsistencies and abrupttask learning, Stock Trading, Cryptocurrency Trading.variations in between episodes. Aside from the already mentionedfield of gaming [10], RL techniques have been first deployed in1 INTRODUCTIONthe field of robotics [7], of personalized recommendations [15] andReinforcement Learning (RL) is a sub-field of Machine Learningresource management [9].specifically designed to handle learning processes for problemsThe problems we are interested in, namely, those related to usingwhich involve a dynamic interaction with a given underlying en-RL for profitable, risk-reduced trading of financial tools based onvironment. In a nutshell, a RL algorithm is supposed to learn tohistorical data, only partially satisfy features F1 & F2. We detailuse the set of observable state variables S (describing the currentthis assertion by outlining three common problems in the field.state of the environment) to take the most appropriate admissibleFirstly, (P1) the reward mechanisms which one might use to judgeaction a. Together with the environment intrinsic stochasticity,the performance of a bot are not robust due the vast amount ofnoise coming from the financial environment. Secondly, (P2) the*These authors contributed equally to this workdefinition of the environment is often subjective, in the sense thatFunded by the European Union Horizon 2020 research and innovation programmeunder the Marie Sklodowska-Curie grant agreement No. 754411one would need to decide which features are to be included in itFunded by the Austrian Research Promotion Agency (FFG)Corresponding Author1']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['multi objective reward generalization improving performanceof deep reinforcement learning selected application instock cryptocurrency tradingfederico cornalba constantin disselkamp sinstitute science technology austriadavide scassola federico cornalba ist ac atchristopher helftrality gmbhconstantin trality comdavide trality comchristopher trality comabstractthe state variable action determine next state thewe investigate potential multi objective deep reinforce environment algorithm visit algorithm ultimatement learning stock cryptocurrency trading specifi goal maximise cumulative reward accounting actionscally build generalized setting la fontaine friedmantaken given episode turn based pre specified reward weighting mechanism specified pri state action reward function r assigning numerical reward r ori embedded learning process complementing itto every pair given state action undertaken algo computational speed ups adding cumulative rewardrithm us several episode accounting exploration ofdiscount factor learning process environment ending pre specified end statefirstly verify resulting multi objective algorithmis reached train action making progressively updatinggeneralizes well provide preliminary statistical evidencethe called q value throughout episode rewardshowing prediction stable correspond function integral part rl algorithm keying single objective strategy secondly show multi metric performance measured objective algorithm clear edge corresponding single intuitive example set problem machine mayobjective strategy reward mechanism sparse e whentackle using rl framework naturally occurs gaming fornon null feedback infrequent time finally discus theinstance atari game state environment isgeneralization property discount factor represented image action algorithm mightthe entirety code provided open source format take correspond directional movement game charac ters reward cumulative game score generally speaking cc conceptsthe vast majority problem rl paradigm origi methodology reinforcement learning neu nally applied exhibit two clear feature firstly f includecomputingrobust well defined reward mechanism provides rlral network multi task learning applied computing eco algorithm necessary quantitative feedback secondly f nomics consistent produceable accessible environment keywordsthis allows rl algorithm thoroughly explore environ ment many episode needed affected deep reinforcement learning multi objective generalization multi potentially destabilizing environment inconsistency abrupttask learning stock trading cryptocurrency trading variation episode aside already mentionedfield gaming rl technique first deployed introductionthe field robotics personalized recommendation andreinforcement learning rl sub field machine learningresource management specifically designed handle learning process problemsthe problem interested namely related usingwhich involve dynamic interaction given underlying en rl profitable risk reduced trading financial tool based onvironment nutshell rl algorithm supposed learn tohistorical data partially satisfy feature f f detailuse set observable state variable describing currentthis assertion outlining three common problem field state environment take appropriate admissiblefirstly p reward mechanism one might use judgeaction together environment intrinsic stochasticity performance bot robust due vast amount ofnoise coming financial environment secondly p author contributed equally workdefinition environment often subjective sense thatfunded european union horizon research innovation programmeunder marie sklodowska curie grant agreement one would need decide feature included itfunded austrian research promotion agency ffg corresponding author']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_text_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "34723e340364e285e2a18e1db349b03f025578f6576f29d6a3007e9cc2a30bbf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
