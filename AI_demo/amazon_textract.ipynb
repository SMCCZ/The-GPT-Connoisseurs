{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import time\n",
    "from trp import Document\n",
    "from botocore.config import Config\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "import calendar\n",
    "import re\n",
    "import nltk\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from transformers import AutoTokenizer,GPT2TokenizerFast\n",
    "\n",
    "import os\n",
    "import openai\n",
    "openai.api_key =os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract text from a image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Text Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_text_extractor(path:str)->str:\n",
    "    '''\n",
    "    This function extracts text from a given image \n",
    "    '''\n",
    "    with open(path, 'rb') as image:\n",
    "        img = bytearray(image.read())\n",
    "        client = boto3.client('textract',region_name='ap-south-1')\n",
    "        response = client.detect_document_text(\n",
    "            Document={'Bytes': img}\n",
    "        )\n",
    "    text = \"\"\n",
    "    for item in response[\"Blocks\"]:\n",
    "        if item[\"BlockType\"] == \"LINE\":\n",
    "            text = text + \" \"+item[\"Text\"]\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = image_text_extractor('/home/cloudcraftz/Desktop/Test_3.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Multi-Objective reward generalization: Improving performance of Deep Reinforcement Learning for selected applications in stock and cryptocurrency trading Federico Cornalba* Constantin Disselkamp**S Institute of Science and Technology Austria Davide Scassola** federico.cornalba@ist.ac.at Christopher Helf TRALITY GmbH constantin@trality.com davide@trality.com christopher@trality.com ABSTRACT the state variables and action (s,a determine the next state of the We investigate the potential of Multi-Objective, Deep Reinforce- environment that the algorithm will visit. The algorithm's ultimate ment Learning for stock and cryptocurrency trading. More specifi- goal is to maximise a cumulative reward (accounting for all actions cally, we build on the generalized setting à la Fontaine and Friedman taken in a given episode), which is in turn based on a pre-specified [6] (where the reward weighting mechanism is not specified a pri- state-action reward function r (assigning a numerical reward r(s, a) ori, but embedded in the learning process) by complementing it to every pair (s, a) of given state and action undertaken). The algo- with computational speed-ups, and adding the cumulative reward's rithm uses several episodes (each accounting for an exploration of discount factor to the learning process. the environment, and each ending when a pre-specified end-state Firstly, we verify that the resulting Multi-Objective algorithm is reached) to train its action-making (by progressively updating generalizes well, and we provide preliminary statistical evidence the so-called Q-values [14] throughout the episodes). The reward showing that its prediction is more stable than the correspond- function is both an integral part of the RL algorithm and the key ing Single-Objective strategy's. Secondly, we show that the Multi- metric against which its performance is measured. Objective algorithm has a clear edge over the corresponding Single- An intuitive example of a set of problems which a machine may Objective strategy when the reward mechanism is sparse (i.e., when tackle using the RL framework naturally occurs in gaming. For non-null feedback is infrequent over time). Finally, we discuss the instance, in the Atari 2600 games [10], the state of environment is generalization properties of the discount factor. represented by an image, the actions which the algorithm might The entirety of our code is provided in open source format. take correspond to directional movements of the game's charac- ters, and rewards are cumulative game scores. Generally speaking, CCS CONCEPTS the vast majority of problems which the RL paradigm was - origi- nally - applied to exhibit two clear features: firstly, (F1) they include Computing methodologies Reinforcement learning; Neu- robust and well-defined reward mechanisms (this provides the RL ral networks; Multi-task learning; Applied computing Eco- algorithms with the necessary quantitative feedback); secondly, (F2) nomics. they have a consistent, re-produceable and accessible environment. KEYWORDS This allows the RL algorithms to thoroughly explore the environ- ment in as many episodes as needed, and not to be affected by - Deep Reinforcement Learning, Multi-Objective Generalization, Multi- potentially destabilizing - environment inconsistencies and abrupt task learning, Stock Trading, Cryptocurrency Trading. variations in between episodes. Aside from the already mentioned field of gaming [10], RL techniques have been first deployed in 1 INTRODUCTION the field of robotics [7], of personalized recommendations [15] and Reinforcement Learning (RL) is a sub-field of Machine Learning resource management [9]. specifically designed to handle learning processes for problems The problems we are interested in, namely, those related to using which involve a dynamic interaction with a given underlying en- RL for profitable, risk-reduced trading of financial tools based on vironment. In a nutshell, a RL algorithm is supposed to learn to historical data, only partially satisfy features F1 & F2. We detail use the set of observable state variables S (describing the current this assertion by outlining three common problems in the field. state of the environment) to take the most appropriate admissible Firstly, (P1) the reward mechanisms which one might use to judge action a. Together with the environment's intrinsic stochasticity, the performance of a bot are not robust due the vast amount of noise coming from the financial environment. Secondly, (P2) the *These authors contributed equally to this work definition of the environment is often subjective, in the sense that Funded by the European Union's Horizon 2020 research and innovation programme under the Marie Sklodowska-Curie grant agreement No. 754411 one would need to decide which features are to be included in it Funded by the Austrian Research Promotion Agency (FFG) Corresponding Author 1\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract text from a document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Text Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_text_extractor(name:str)->list:\n",
    "    ''' \n",
    "    This function takes the name of the document stored in AWS S3 and returns the actual text, the extracted text in a list.\n",
    "    '''\n",
    "\n",
    "    def InvokeTextDetectJob(s3BucketName:str, objectName:str):\n",
    "        response = None\n",
    "        client = boto3.client('textract',region_name='ap-south-1')\n",
    "        \n",
    "        \n",
    "        response = client.start_document_text_detection(\n",
    "                DocumentLocation={\n",
    "                        'S3Object': {\n",
    "                                        'Bucket': s3BucketName,\n",
    "                                        'Name': objectName,\n",
    "                                    }\n",
    "            })\n",
    "        return response[\"JobId\"]\n",
    "\n",
    "\n",
    "    def CheckJobComplete(jobId):\n",
    "        time.sleep(5)\n",
    "        client = boto3.client('textract',region_name='ap-south-1')\n",
    "        response = client.get_document_text_detection(JobId=jobId)\n",
    "        status = response[\"JobStatus\"]\n",
    "        print(\"Job status: {}\".format(status))\n",
    "        while(status == \"IN_PROGRESS\"):\n",
    "            time.sleep(5)\n",
    "            response = client.get_document_text_detection(JobId=jobId)\n",
    "            status = response[\"JobStatus\"]\n",
    "            print(\"Job status: {}\".format(status))\n",
    "        return status\n",
    "\n",
    "\n",
    "    def JobResults(jobId):\n",
    "        pages = []\n",
    "        client = boto3.client('textract',region_name='ap-south-1')\n",
    "        response = client.get_document_text_detection(JobId=jobId)\n",
    "    \n",
    "        pages.append(response)\n",
    "        print(\"Resultset page recieved: {}\".format(len(pages)))\n",
    "        nextToken = None\n",
    "        if('NextToken' in response):\n",
    "            nextToken = response['NextToken']\n",
    "            while(nextToken):\n",
    "                response = client.get_document_text_detection(JobId=jobId, NextToken=nextToken)\n",
    "                pages.append(response)\n",
    "                print(\"Resultset page recieved: {}\".format(len(pages)))\n",
    "                nextToken = None\n",
    "                if('NextToken' in response):\n",
    "                    nextToken = response['NextToken']\n",
    "        return pages    \n",
    "\n",
    "\n",
    "    extracted_text = []\n",
    "\n",
    "    # S3 Document Data\n",
    "    s3BucketName = \"fintech-sentiment-textract\"\n",
    "    documentName = name\n",
    "\n",
    "    # Function invokation\n",
    "    jobId = InvokeTextDetectJob(s3BucketName, documentName)\n",
    "    print(f\"Started job with id: {jobId} , name : {name}\")\n",
    "\n",
    "    if(CheckJobComplete(jobId)):\n",
    "        response = JobResults(jobId)\n",
    "        text_out = ''\n",
    "        for resultPage in response:\n",
    "            for item in resultPage[\"Blocks\"]:\n",
    "                if item[\"BlockType\"] == \"LINE\":\n",
    "                    text_out = text_out + item[\"Text\"]\n",
    "                    text_out = text_out.replace(\"\\'s\",\"\")\n",
    "\n",
    "    print(f\"For {name}, Text Extracted\")\n",
    "    extracted_text.append(text_out)\n",
    "\n",
    "    return extracted_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started job with id: f90ec13d3a0bfe70d5fd7f91158d77156169096aea1d7985323a1dd5a5b20d92 , name : RL-1.pdf\n",
      "Job status: IN_PROGRESS\n",
      "Job status: SUCCEEDED\n",
      "Resultset page recieved: 1\n",
      "For RL-1.pdf, Text Extracted\n"
     ]
    }
   ],
   "source": [
    "extracted_text = document_text_extractor('RL-1.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Multi-Objective reward generalization: Improving performanceof Deep Reinforcement Learning for selected applications instock and cryptocurrency tradingFederico Cornalba*Constantin Disselkamp**SInstitute of Science and Technology AustriaDavide Scassola**federico.cornalba@ist.ac.atChristopher HelfTRALITY GmbHconstantin@trality.comdavide@trality.comchristopher@trality.comABSTRACTthe state variables and action (s, determine the next state of theWe investigate the potential of Multi-Objective, Deep Reinforce-environment that the algorithm will visit. The algorithm ultimatement Learning for stock and cryptocurrency trading. More specifi-goal is to maximise a cumulative reward (accounting for all actionscally, we build on the generalized setting à la Fontaine and Friedmantaken in a given episode), which is in turn based on a pre-specified[6] (where the reward weighting mechanism is not specified a pri-state-action reward function r (assigning a numerical reward r(s, a)ori, but embedded in the learning process) by complementing itto every pair (s, a) of given state and action undertaken). The algo-with computational speed-ups, and adding the cumulative rewardrithm uses several episodes (each accounting for an exploration ofdiscount factor to the learning process.the environment, and each ending when a pre-specified end-stateFirstly, we verify that the resulting Multi-Objective algorithmis reached) to train its action-making (by progressively updatinggeneralizes well, and we provide preliminary statistical evidencethe so-called Q-values [14] throughout the episodes). The rewardshowing that its prediction is more stable than the correspond-function is both an integral part of the RL algorithm and the keying Single-Objective strategy. Secondly, we show that the Multi-metric against which its performance is measured.Objective algorithm has a clear edge over the corresponding Single-An intuitive example of a set of problems which a machine mayObjective strategy when the reward mechanism is sparse (i.e., whentackle using the RL framework naturally occurs in gaming. Fornon-null feedback is infrequent over time). Finally, we discuss theinstance, in the Atari 2600 games [10], the state of environment isgeneralization properties of the discount factor.represented by an image, the actions which the algorithm mightThe entirety of our code is provided in open source format.take correspond to directional movements of the game charac-ters, and rewards are cumulative game scores. Generally speaking,CCS CONCEPTSthe vast majority of problems which the RL paradigm was - origi-methodologies Reinforcement learning; Neu-nally - applied to exhibit two clear features: firstly, (F1) they includeComputingrobust and well-defined reward mechanisms (this provides the RLral networks; Multi-task learning; Applied computing Eco-algorithms with the necessary quantitative feedback); secondly, (F2)nomics.they have a consistent, re-produceable and accessible environment.KEYWORDSThis allows the RL algorithms to thoroughly explore the environ-ment in as many episodes as needed, and not to be affected by -Deep Reinforcement Learning, Multi-Objective Generalization, Multi-potentially destabilizing - environment inconsistencies and abrupttask learning, Stock Trading, Cryptocurrency Trading.variations in between episodes. Aside from the already mentionedfield of gaming [10], RL techniques have been first deployed in1 INTRODUCTIONthe field of robotics [7], of personalized recommendations [15] andReinforcement Learning (RL) is a sub-field of Machine Learningresource management [9].specifically designed to handle learning processes for problemsThe problems we are interested in, namely, those related to usingwhich involve a dynamic interaction with a given underlying en-RL for profitable, risk-reduced trading of financial tools based onvironment. In a nutshell, a RL algorithm is supposed to learn tohistorical data, only partially satisfy features F1 & F2. We detailuse the set of observable state variables S (describing the currentthis assertion by outlining three common problems in the field.state of the environment) to take the most appropriate admissibleFirstly, (P1) the reward mechanisms which one might use to judgeaction a. Together with the environment intrinsic stochasticity,the performance of a bot are not robust due the vast amount ofnoise coming from the financial environment. Secondly, (P2) the*These authors contributed equally to this workdefinition of the environment is often subjective, in the sense thatFunded by the European Union Horizon 2020 research and innovation programmeunder the Marie Sklodowska-Curie grant agreement No. 754411one would need to decide which features are to be included in itFunded by the Austrian Research Promotion Agency (FFG)Corresponding Author1']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trained_answer(question:str)->str:\n",
    "    ''' \n",
    "    This function provides answer to a question from the data it has been trained on.\n",
    "    '''\n",
    "\n",
    "    COMPLETIONS_MODEL = \"text-davinci-003\"\n",
    "    answer=openai.Completion.create(\n",
    "        prompt=question,\n",
    "        temperature=0,\n",
    "        max_tokens=300,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        model=COMPLETIONS_MODEL\n",
    "    )[\"choices\"][0][\"text\"].strip(\" \\n\")\n",
    "    ModelAnswer=answer\n",
    "\n",
    "    return ModelAnswer    \n",
    "\n",
    "\n",
    "def answer_the_question(text:str,question:str)->str:  \n",
    "    ''' \n",
    "    This function answers a question based on the given context.\n",
    "    '''  \n",
    "    \n",
    "    COMPLETIONS_MODEL = \"text-davinci-003\"\n",
    "    context=text\n",
    "    prompt = f\"\"\"Answer the question as truthfully as possible with structured compact english sentences using the provided text, and if the answer is not contained within the text below, say \"not_found\"\n",
    "        Context:\n",
    "        '{context}'\n",
    "        Q: \"{question}\"?\n",
    "        A:\"\"\"\n",
    "    answer= openai.Completion.create(\n",
    "        prompt=prompt,\n",
    "        temperature=0,\n",
    "        max_tokens=300,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        model=COMPLETIONS_MODEL\n",
    "        )[\"choices\"][0][\"text\"].strip(\" \\n\")\n",
    "    \n",
    "    if answer=='Not_found.' or answer == 'not_found':\n",
    "        return trained_answer(question)\n",
    "    else:\n",
    "        return answer\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The Sharpe ratio is a measure of risk-adjusted return, which compares the return of an investment to that of an alternative investment with similar risk. It is calculated by subtracting the risk-free rate from the rate of return of the investment and dividing the result by the standard deviation of the investment's returns. The higher the Sharpe ratio, the better the investment's performance has been relative to the amount of risk taken.\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_the_question(text,'What is sharpe ratio?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This paper investigates the potential of Multi-Objective, Deep Reinforcement Learning for stock and cryptocurrency trading. It builds on the generalized setting à la Fontaine and Friedman by complementing it with computational speed-ups, and adding the cumulative reward's discount factor to the learning process. The paper verifies that the resulting Multi-Objective algorithm generalizes well and has a clear edge over the corresponding Single-Objective strategy when the reward mechanism is sparse. The code is provided in open source format.\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_the_question(text,'Give me a summary of this paper?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cryptocurrency trading is the buying and selling of digital currencies, such as Bitcoin, Ethereum, and Litecoin, in order to generate a profit.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_the_question(text,'What is cryptocurrency trading ?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "34723e340364e285e2a18e1db349b03f025578f6576f29d6a3007e9cc2a30bbf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
